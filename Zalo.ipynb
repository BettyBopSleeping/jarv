{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BettyBopSleeping/jarv/blob/main/Zalo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SrTZQceZKXNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "training_data = [\n",
        "    {\"prompt\": \"User: What is a resistor?\\n\", \"completion\": \"Zalo: A resistor is a passive component that resists the flow of electric current. Itâ€™s used to control voltage and current in circuits.\\n\"},\n",
        "{\"prompt\": \"User: What's the purpose of a capacitor?\\n\", \"completion\": \"Zalo: A capacitor stores electrical energy temporarily. Itâ€™s great for filtering signals or supplying short bursts of power.\\n\"},\n",
        "{\"prompt\": \"User: How does a diode work?\\n\", \"completion\": \"Zalo: A diode only allows current to flow in one direction. Itâ€™s like a one-way valve for electricity.\\n\"},\n",
        "{\"prompt\": \"User: Can you explain what a transistor does?\\n\", \"completion\": \"Zalo: A transistor acts like a switch or amplifier. It can control large currents with a small input signal.\\n\"},\n",
        "{\"prompt\": \"User: Whatâ€™s the difference between AC and DC?\\n\", \"completion\": \"Zalo: AC is alternating currentâ€”it changes direction. DC is direct currentâ€”it flows in one direction only. Think wall socket vs battery.\\n\"},\n",
        "{\"prompt\": \"User: What is voltage?\\n\", \"completion\": \"Zalo: Voltage is the electric potential difference. Itâ€™s like the pressure that pushes electrons through a wire.\\n\"},\n",
        "{\"prompt\": \"User: What is current?\\n\", \"completion\": \"Zalo: Current is the flow of electric charge. Measured in amps, it tells you how much electricity is moving.\\n\"},\n",
        "{\"prompt\": \"User: What is resistance?\\n\", \"completion\": \"Zalo: Resistance opposes current flow. It's like friction in a pipeâ€”more resistance, less current.\\n\"},\n",
        "{\"prompt\": \"User: Whatâ€™s Ohmâ€™s Law?\\n\", \"completion\": \"Zalo: Ohmâ€™s Law states that V = I Ã— R. Voltage equals current times resistance. It's foundational in circuit analysis.\\n\"},\n",
        "{\"prompt\": \"User: What is an LED?\\n\", \"completion\": \"Zalo: An LED is a Light Emitting Diode. It lights up when current flows through it in the right direction.\\n\"},\n",
        "{\"prompt\": \"User: What is a breadboard?\\n\", \"completion\": \"Zalo: A breadboard is a reusable platform for testing circuits without soldering. Itâ€™s every inventorâ€™s playground.\\n\"},\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "data_path = \"/content/training_data.jsonl\"\n",
        "with open(data_path, \"a\", encoding=\"utf-8\") as outfile:\n",
        "    for entry in training_data:\n",
        "        json.dump(entry, outfile, ensure_ascii=False)\n",
        "        outfile.write(\"\\n\")\n",
        "\n",
        "validation_data = [\n",
        "    {\"prompt\": \"User: What is a resistor?\\n\", \"completion\": \"Zalo: A resistor is a passive component that resists the flow of electric current. Itâ€™s used to control voltage and current in circuits.\\n\"},\n",
        "{\"prompt\": \"User: What's the purpose of a capacitor?\\n\", \"completion\": \"Zalo: A capacitor stores electrical energy temporarily. Itâ€™s great for filtering signals or supplying short bursts of power.\\n\"},\n",
        "{\"prompt\": \"User: How does a diode work?\\n\", \"completion\": \"Zalo: A diode only allows current to flow in one direction. Itâ€™s like a one-way valve for electricity.\\n\"},\n",
        "{\"prompt\": \"User: Can you explain what a transistor does?\\n\", \"completion\": \"Zalo: A transistor acts like a switch or amplifier. It can control large currents with a small input signal.\\n\"},\n",
        "{\"prompt\": \"User: Whatâ€™s the difference between AC and DC?\\n\", \"completion\": \"Zalo: AC is alternating currentâ€”it changes direction. DC is direct currentâ€”it flows in one direction only. Think wall socket vs battery.\\n\"},\n",
        "{\"prompt\": \"User: What is voltage?\\n\", \"completion\": \"Zalo: Voltage is the electric potential difference. Itâ€™s like the pressure that pushes electrons through a wire.\\n\"},\n",
        "{\"prompt\": \"User: What is current?\\n\", \"completion\": \"Zalo: Current is the flow of electric charge. Measured in amps, it tells you how much electricity is moving.\\n\"},\n",
        "{\"prompt\": \"User: What is resistance?\\n\", \"completion\": \"Zalo: Resistance opposes current flow. It's like friction in a pipeâ€”more resistance, less current.\\n\"},\n",
        "{\"prompt\": \"User: Whatâ€™s Ohmâ€™s Law?\\n\", \"completion\": \"Zalo: Ohmâ€™s Law states that V = I Ã— R. Voltage equals current times resistance. It's foundational in circuit analysis.\\n\"},\n",
        "{\"prompt\": \"User: What is an LED?\\n\", \"completion\": \"Zalo: An LED is a Light Emitting Diode. It lights up when current flows through it in the right direction.\\n\"},\n",
        "{\"prompt\": \"User: What is a breadboard?\\n\", \"completion\": \"Zalo: A breadboard is a reusable platform for testing circuits without soldering. Itâ€™s every inventorâ€™s playground.\\n\"},\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "data_path = \"/content/validation_data.jsonl\"\n",
        "with open(data_path, \"a\", encoding=\"utf-8\") as outfile:\n",
        "    for entry in validation_data:\n",
        "        json.dump(entry, outfile, ensure_ascii=False)\n",
        "        outfile.write(\"\\n\")"
      ],
      "metadata": {
        "id": "TlmertZPQfba"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download wordnet for data augmentation\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Create a custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = item['input_ids'].clone()  # For language modeling, labels are the same as inputs\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# Simple word replacement augmentation function\n",
        "def augment_text(text, prob=0.1):\n",
        "    words = text.split()\n",
        "    result = []\n",
        "\n",
        "    for word in words:\n",
        "        if random.random() < prob and len(word) > 3:  # Only replace some words\n",
        "            synonyms = []\n",
        "            for syn in wordnet.synsets(word):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonyms.append(lemma.name())\n",
        "\n",
        "            if synonyms:\n",
        "                result.append(random.choice(synonyms))\n",
        "            else:\n",
        "                result.append(word)\n",
        "        else:\n",
        "            result.append(word)\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs('./results', exist_ok=True)\n",
        "os.makedirs('./logs', exist_ok=True)\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "print(\"Loading pre-trained model and tokenizer...\")\n",
        "model_name = 'gpt2'  # You can change this to 'gpt2-medium' for better results if you have enough resources\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the tokenizer has the pad token set\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading training and validation data...\")\n",
        "# Load the training data\n",
        "with open('/content/training_data.jsonl', 'r') as file:\n",
        "    cleaned_data = [json.loads(line) for line in file]\n",
        "\n",
        "# Load the validation data\n",
        "with open('/content/validation_data.jsonl', 'r') as file:\n",
        "    validation_data = [json.loads(line) for line in file]\n",
        "\n",
        "# Extract and format prompts from training data\n",
        "train_prompts = [f\"<{entry['prompt']}\" for entry in cleaned_data if \"prompt\" in entry]\n",
        "\n",
        "# Extract and format prompts from validation data\n",
        "validation_prompts = [f\"{entry['prompt']}\" for entry in validation_data if \"prompt\" in entry]\n",
        "\n",
        "# Data augmentation for training set\n",
        "print(\"Performing data augmentation...\")\n",
        "augmented_prompts = []\n",
        "for prompt in tqdm(train_prompts):\n",
        "    # Add original prompt\n",
        "    augmented_prompts.append(prompt)\n",
        "    # Add augmented version\n",
        "    try:\n",
        "        augmented_prompts.append(augment_text(prompt))\n",
        "    except:\n",
        "        pass  # Skip if augmentation fails\n",
        "\n",
        "# Use augmented data for training\n",
        "train_prompts = augmented_prompts\n",
        "print(f\"Training data size after augmentation: {len(train_prompts)}\")\n",
        "\n",
        "# Tokenize training data\n",
        "print(\"Tokenizing data...\")\n",
        "train_encodings = tokenizer(\n",
        "    train_prompts,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors='np',\n",
        "    return_attention_mask=True\n",
        ")\n",
        "\n",
        "# Tokenize validation data\n",
        "validation_encodings = tokenizer(\n",
        "    validation_prompts,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors='np',\n",
        "    return_attention_mask=True\n",
        ")\n",
        "\n",
        "# Create proper dataset objects\n",
        "train_dataset = TextDataset(train_encodings)\n",
        "validation_dataset = TextDataset(validation_encodings)\n",
        "\n",
        "# Define training arguments with improved parameters\n",
        "print(\"Setting up training arguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"steps\",      # Evaluate during training\n",
        "    eval_steps=100,                   # Evaluate every 100 steps\n",
        "    learning_rate=2e-5,               # Standard learning rate for fine-tuning\n",
        "    per_device_train_batch_size=4,    # Batch size per device during training\n",
        "    gradient_accumulation_steps=4,    # Simulate larger batch sizes\n",
        "    per_device_eval_batch_size=4,     # Batch size for evaluation\n",
        "    weight_decay=0.01,                # Weight decay for regularization\n",
        "    num_train_epochs=5,               # Number of training epochs\n",
        "    logging_dir='./logs',             # Directory for logs\n",
        "    logging_steps=10,                 # Log every 10 steps\n",
        "    save_steps=100,                   # Save checkpoint every 100 steps\n",
        "    save_total_limit=3,               # Keep only the 3 best checkpoints\n",
        "    load_best_model_at_end=True,      # Load the best model at the end\n",
        "    metric_for_best_model=\"eval_loss\", # Use loss as the metric\n",
        "    greater_is_better=False,          # Lower loss is better\n",
        "    warmup_ratio=0.1,                 # Warm up over 10% of training\n",
        "    fp16=True,                        # Use mixed precision if available\n",
        "    report_to=\"tensorboard\",          # Report to TensorBoard\n",
        ")\n",
        "\n",
        "# Add early stopping\n",
        "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "\n",
        "# Prepare the Trainer\n",
        "print(\"Initializing trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    callbacks=[early_stopping_callback],\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "# Save the trained model and tokenizer\n",
        "print(\"Saving the fine-tuned model and tokenizer...\")\n",
        "model.save_pretrained('./final_model')\n",
        "tokenizer.save_pretrained('./final_model')\n",
        "print(\"Fine-tuning complete! Model and tokenizer saved to ./final_model\")\n",
        "\n",
        "def generate_text(prompt, max_length=100):\n",
        "    # Get the device the model is on\n",
        "    device = model.device\n",
        "\n",
        "    # Encode and move to the same device as the model\n",
        "    input_ids = tokenizer.encode(f\"{prompt}\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,  # Prevent repeating 3-grams\n",
        "        repetition_penalty=1.5\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(output[0])\n",
        "    return generated_text\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.3,\n",
        "        top_k=49,\n",
        "        top_p=0.98,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Test the model with a few examples\n",
        "print(\"\\nTesting the fine-tuned model with example prompts:\")\n",
        "test_prompts = [\n",
        "    \"Who made Ohm's Law?\",\n",
        "    \"What is a transistor\",\n",
        "    \"What is a LED?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text(prompt)\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Generated: {generated}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "GfYmNbMw7fUU",
        "outputId": "4acd014e-8767-4096-df4f-ad235914553e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained model and tokenizer...\n",
            "Loading training and validation data...\n",
            "Performing data augmentation...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:08<00:00,  2.62it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data size after augmentation: 44\n",
            "Tokenizing data...\n",
            "Setting up training arguments...\n",
            "Initializing trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2/10 : < :, Epoch 0.36/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4/10 04:16 < 12:49, 0.01 it/s, Epoch 1/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODf+oqqCRvMeBCt+Lt4oe2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}